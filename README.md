<img src=./images/logo-row.svg />

<div align="center">

# Tile Language for Ascend

</div>

<!-- Tile Language (**tile-lang**) is a concise domain-specific language designed to streamline the development of high-performance GPU/CPU kernels (e.g., GEMM, Dequant GEMM, FlashAttention, LinearAttention). By employing a Pythonic syntax with an underlying compiler infrastructure on top of [TVM](https://tvm.apache.org/), tile-lang allows developers to focus on productivity without sacrificing the low-level optimizations necessary for state-of-the-art performance. -->
<!-- TileLang for Ascend æ˜¯ Tile Languageï¼ˆtile-langï¼‰åœ¨æ˜‡è…¾ï¼ˆAscendï¼‰AIå¤„ç†å™¨ä¸Šçš„æ·±åº¦é€‚é…ä¸Žæ‰©å±•ã€‚ä½œä¸ºä¸€å¥—ç®€æ´é«˜æ•ˆçš„é¢†åŸŸç‰¹å®šè¯­è¨€ï¼ŒTileLang åŽŸç”Ÿæ”¯æŒä»¥ç±» Python è¯­æ³•ç¼–å†™é«˜æ€§èƒ½è®¡ç®—æ ¸å‡½æ•°ï¼ˆå¦‚ GEMMã€Dequant GEMMã€FlashAttentionã€LinearAttention ç­‰ï¼‰ï¼Œå¹¶ä¾æ‰˜ TVM ä¸Ž MLIR æž„å»ºå¼ºå¤§çš„ç¼–è¯‘åŸºç¡€è®¾æ–½ã€‚

åœ¨ TileLang ç”Ÿæ€ä¸­ï¼Œæˆ‘ä»¬æž„å»ºäº†é¢å‘ Ascend çš„ NPU ä¸­é—´è¡¨ç¤ºï¼ˆNPUIRï¼‰åŸºç¡€è®¾æ–½ï¼Œä½¿å…¶èƒ½å¤Ÿæ— ç¼èžå…¥åŸºäºŽ MLIR çš„å¼€æº AI ç¼–è¯‘å™¨ç”Ÿæ€ã€‚è¿™ä¸€ä¸¾æŽªä¸ä»…æå‡äº†ç¼–è¯‘æ ˆçš„å¼€æ”¾æ€§ä¸Žå¯æ‰©å±•æ€§ï¼Œä¹Ÿä¸ºå¼€å‘è€…æä¾›äº†æ›´çµæ´»ã€é«˜æ•ˆçš„ç®—å­å¼€å‘è·¯å¾„ã€‚ -->
TileLang for Ascend is a deep adaptation and extension of Tile Language (tile-lang) tailored for the Ascend AI processor. As a concise and efficient domain-specific language, TileLang natively supports writing high-performance compute kernelsâ€”such as GEMM, Dequantized GEMM, FlashAttention, and LinearAttentionâ€”using Python-like syntax, and leverages TVM and MLIR to build a robust compiler infrastructure.

Within the TileLang ecosystem, we have developed an NPU Intermediate Representation (NPUIR) infrastructure specifically for Ascend, enabling seamless integration into the open-source AI compiler ecosystem based on MLIR. This effort not only enhances the openness and extensibility of the compiler stack but also provides developers with a more flexible and efficient pathway for custom operator development.

<!-- ascend NPU IRæŠ€æœ¯è·¯çº¿ for tilelang -->

<img src=./images/MatmulExample.png />
<div align="center">
<img src=./images/npuir_architecture.png style="width: 50%";/>
</div>


## Latest News
- 26/09/2025 ðŸš€: Officially establish the NPU Intermediate Representation (NPUIR) infrastructure for Ascend within the TileLang ecosystem, deeply integrating into the open-source AI compiler ecosystem based on MLIR. At the same time, deliver peak performanceâ€”fusion operators such as FlashAttention (FA) written in TileLang achieve performance on Ascend hardware that matches hand-written AscendC equivalents at a 1.0x level, balancing both development efficiency and ultimate performance!
<!-- - åœ¨ TileLang ç”Ÿæ€ä¸­æ­£å¼æž„å»ºé¢å‘æ˜‡è…¾ï¼ˆAscendï¼‰çš„ NPU ä¸­é—´è¡¨ç¤ºï¼ˆNPUIRï¼‰åŸºç¡€è®¾æ–½ï¼Œæ·±åº¦èžå…¥åŸºäºŽ MLIR çš„å¼€æº AI ç¼–è¯‘å™¨ç”Ÿæ€ï¼›åŒæ—¶é‡Šæ”¾æžè‡´æ€§èƒ½â€”â€”ä½¿ç”¨ TileLang ç¼–å†™çš„ FlashAttentionï¼ˆFAï¼‰ç­‰èžåˆç®—å­ï¼Œåœ¨æ˜‡è…¾ç¡¬ä»¶ä¸Šæ€§èƒ½è¾¾åˆ° AscendC æ‰‹å†™ç­‰ä»·ç®—å­çš„ 1.0x æ°´å¹³ï¼Œå…¼é¡¾å¼€å‘æ•ˆçŽ‡ä¸Žæžè‡´æ€§èƒ½ï¼ -->
- 14/04/2025 ðŸš€: Added high-performance FlashMLA implementation for AMD MI300X, achieving performance parity with hand-optimized assembly kernels of Aiter! See [example_mla_amd](./examples/deepseek_mla/amd/README.md) for details.
- 03/03/2025 ðŸš€: Added high-performance MLA Decoding support using only 80 lines of Python code, achieving performance on par with FlashMLA on H100 (see [example_mla_decode.py](./examples/deepseek_mla/example_mla_decode.py))! We also provide [documentation](./examples/deepseek_mla/README.md) explaining how TileLang achieves this.
- 02/15/2025 âœ¨: Added WebGPU Codegen support, see [Pull Request #86](https://github.com/tile-ai/tilelang/pull/86)!
- 02/12/2025 âœ¨: Excited to announce the release of [v0.1.0](https://github.com/tile-ai/tilelang/releases/tag/v0.1.0)!
- 02/10/2025 ðŸš€: Added debug tools for TileLangâ€”`T.print` for printing variables/buffers ([docs](https://tilelang.com/tutorials/debug_tools_for_tilelang.html)) and a memory layout plotter ([examples/plot_layout](./examples/plot_layout)).
- 01/20/2025 âœ¨: We are excited to announce that tile-lang, a dsl for high performance AI workloads, is now open source and available to the public!

## Tested Devices
<!-- Although tile-lang aims to be portable across a range of Devices, it has been specifically tested and validated on the following devices: for NVIDIA GPUs, this includes the H100 (with Auto TMA/WGMMA support), A100, V100, RTX 4090, RTX 3090, and RTX A6000; for AMD GPUs, it includes the MI250 (with Auto MatrixCore support) and the MI300X (with Async Copy support). -->
<!-- å°½ç®¡ TileLang æ—¨åœ¨æ”¯æŒå¤šç§è®¾å¤‡çš„å¯ç§»æ¤æ€§ï¼Œä½†å®ƒå·²åœ¨ä»¥ä¸‹è®¾å¤‡ä¸Šç»è¿‡ä¸“é—¨æµ‹è¯•å’ŒéªŒè¯ï¼š

åŽä¸ºæ˜‡è…¾ AI åŠ é€Ÿå¡ï¼šåŒ…æ‹¬åŸºäºŽ Ascend 310 çš„æŽ¨ç†å¡å’ŒåŸºäºŽ Ascend 910 çš„è®­ç»ƒå¡ã€‚ -->
Although TileLang aims to support portability across a variety of devices, it has been specifically tested and validated on the following hardware:Huawei Ascend AI accelerators,including Ascend 310-based inference cards and Ascend 910-based training cards.


## OP Implementation Examples
**tile-lang** provides the building blocks to implement a wide variety of operators. Some examples include:

- [Vector Add](./docs/tutorials/vec_add_tutorials.md)
- [Matrix Multiplication](./examples/gemm/)
- [Dequantization GEMM](./examples/dequantize_gemm/)
- [Flash Attention](./examples/flash_attention/)
- [Flash Linear Attention](./examples/linear_attention/)
- [Flash MLA Decoding](./examples/deepseek_mla/)
- [Native Sparse Attention](./examples/native_sparse_attention/)

Within the `examples` directory, you will also find additional complex kernelsâ€”such as convolutions, forward/backward passes for FlashAttention, more operators will continuously be added.


## Benchmark Summary

TileLang achieves exceptional performance across a variety of computational patterns. Comprehensive benchmark scripts and settings are available at [tilelang-benchmark](https://github.com/tile-ai/tilelang-benchmark). Below are selected results showcasing its capabilities:

- MLA Decoding Performance on H100

  <div style="display: flex; gap: 10px; justify-content: center;">
    <div style="flex: 1;">
      <img src="./examples/deepseek_mla/figures/bs64_float16.png" alt="mla decode performance bs64 on H100" width="100%" />
    </div>
    <div style="flex: 1;">
      <img src="./examples/deepseek_mla/figures/bs128_float16.png" alt="mla decode performance bs128 on H100" width="100%" />
    </div>
  </div>
  
- Flash Attention Performance on H100

  <div align="center">    <img src="./images/mha_performance_h100.png" alt="operator performance on H100" width=80% />
  </div>

- Matmul Performance on GPUs (RTX 4090, A100, H100, MI300X)

  <div>
    <img src="./images/op_benchmark_consistent_gemm_fp16.png" alt="gemm fp16 performance on Gpus" />
  </div>

- Dequantize Matmul Performance on A100

  <div>
    <img src="./images/op_benchmark_a100_wq_gemv.png" alt="dequantize gemv performance on A100" />
  </div>

## Installation
### Method 1: Install from source
### Environment Setup

Install the Ascend Toolkit.
<!-- #### çŽ¯å¢ƒå‡†å¤‡

å‡†å¤‡Ascend-toolkit -->


<!-- [ä¸‹è½½å®‰è£…åŒ…](https://www.hiascend.com/developer/download/community/result?cann=8.3.RC1.alpha002)ï¼Œå®‰è£…`Ascend-cann-toolkit`ã€‚å®Œæ•´å®‰è£…æ­¥éª¤å‚è€ƒ[ç›¸å…³æ–‡æ¡£](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/83RC1alpha002/softwareinst/instg/instg_0008.html?Mode=PmIns&OS=Debian&Software=cannToolKit)ã€‚ -->

[Download the installation package](https://www.hiascend.com/developer/download/community/result?cann=8.3.RC1.alpha002)ï¼Œinstall`Ascend-cann-toolkit`.For complete installation instructions, refer to the [relevant documentation](https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/83RC1alpha002/softwareinst/instg/instg_0008.html?Mode=PmIns&OS=Debian&Software=cannToolKit).

```shell
chmod +x Ascend-cann-toolkit_{ascend-cann-toolkit version}_linux-aarch64.run
./Ascend-cann-toolkit_{ascend-cann-toolkit version}_linux-aarch64.run --install
```

<!-- é…ç½®çŽ¯å¢ƒå˜é‡ï¼š -->
Configure environment variables:

```
source /path/to/install/Ascend/ascend-toolkit/set_env.sh
```

<!-- å‡†å¤‡pythonçŽ¯å¢ƒï¼Œæ»¡è¶³Pythonç‰ˆæœ¬ä¸º3.7.*x*è‡³3.11.4ï¼Œä¸”å…·æœ‰pip3 -->
Prepare a Python environment with Python version between 3.7.*x* and 3.11.4 (inclusive) and ensure that `pip3` is available.


   Ascend Toolkit Installation Requirements

   ```shell
   pip3 install attrs cython 'numpy>=1.19.2,<=1.24.0' decorator sympy cffi pyyaml pathlib2 psutil protobuf==3.20.0 scipy requests absl-py
   ```

<!-- éƒ¨ç½²Bishengç¼–è¯‘å™¨: -->
Deploy the Bisheng compiler:


   ```shell
   export BISHENG_INSTALL_PATH=/path/to/bishengir-compile
   ```
   <!-- è¡¥å……çŽ¯å¢ƒå˜é‡è®¾ç½® -->
   Set Environment Variables

```shell
export ACL_OP_INIT_MODE=1
```
  <!-- æ³¨æ„ï¼šå¦‚æžœç”¨æˆ·éœ€è¦æ–°çš„ç¼–è¯‘å™¨å®‰è£…åŒ…ï¼Œè¯·è”ç³»ç¤¾åŒºç®¡ç†å‘˜zhaojiqiao@huawei.com,yangsichan@huawei.com TEL:15901269653 -->

  Note: If you require a new compiler installation package, please contact the community administrators:  
**zhaojiqiao@huawei.com**, **yangsichan@huawei.com**  


   

#### Build

<!-- æ‹‰å–ä»£ç  -->
Pull the code

```shell
git clone https://github.com/xxxx/tilelang-npu.git
```

<!-- æ‰§è¡Œå®‰è£…è„šæœ¬ -->
Run the installation script

```shell
cd tilelang-npu
chmod +x ./install_npuir.sh
./install_npuir.sh
```

Install torch_npu

```shell
pip install pybind11 torch_npu
```



<!-- ### Method 1: Install with Pip

The quickest way to get started is to install the latest release from PyPI:

```bash
pip install tilelang
```

Alternatively, you can install directly from the GitHub repository:

```bash
pip install git+https://github.com/tile-ai/tilelang
```

Or install locally:

```bash
# install required system dependencies
sudo apt-get update
sudo apt-get install -y python3-setuptools gcc libtinfo-dev zlib1g-dev build-essential cmake libedit-dev libxml2-dev

pip install -e . -v # remove -e option if you don't want to install in editable mode, -v for verbose output
```

### Method 2: Build from Source
We currently provide three ways to install **tile-lang** from source:
 - [Install from Source (using your own TVM installation)](./docs/get_started/Installation.md#method-1-install-from-source-using-your-own-tvm-installation)
 - [Install from Source (using the bundled TVM submodule)](./docs/get_started/Installation.md#method-2-install-from-source-using-the-bundled-tvm-submodule)
 - [Install Using the Provided Script](./docs/get_started/Installation.md#method-3-install-using-the-provided-script)

### Method 3: Install with Nightly Version

For users who want access to the latest features and improvements before official releases, we provide nightly builds of **tile-lang**.

```bash
pip install tilelang -f https://tile-ai.github.io/whl/nightly/cu121/
# or pip install tilelang --find-links https://tile-ai.github.io/whl/nightly/cu121/
``` -->

<!-- > **Note:** Nightly builds contain the most recent code changes but may be less stable than official releases. They're ideal for testing new features or if you need a specific bugfix that hasn't been released yet. -->

## Quick Start

This code implements a vector addition kernel using TileLang, a domain-specific language for NPU (Neural Processing Unit) programming. It defines a parallel kernel that adds two float32 vectors of length 4096 on the NPU by loading data into on-chip unified buffers, performing element-wise addition via a low-level NPU instruction (`npuir_add`), and writing the result back to global memory. The test function compares the kernelâ€™s output against PyTorchâ€™s native vector addition to verify correctness. The example runs on NPU device 6 and demonstrates basic TileLang workflow: kernel definition, compilation to NPU IR, and execution with PyTorch tensors.

```python
# Copyright (c) Tile-AI Corporation.
# Licensed under the MIT License.

import os

import tilelang
import tilelang.language as T  # Import TileLang DSL for kernel definition

import torch
import torch_npu  # Import NPU (Neural Processing Unit) backend support for PyTorch

# Clear any previously cached compiled kernels to ensure a clean run
tilelang.cache.clear_cache()

# Define data type and sequence length for the vector addition
dtype = "float32"
seq_len = 4096  # Length of the vectors to be added

def vec_add(N, block_N, dtype="float32"):
    """
    Define a vector addition kernel using TileLang.
    
    Parameters:
    - N: Total length of the vectors.
    - block_N: Number of elements processed per kernel thread/block.
    - dtype: Data type of the tensors (default: "float32").
    
    Returns:
    - A TileLang prim_func representing the vector addition kernel.
    """
    n_num = N // block_N  # Number of blocks (each block processes `block_N` elements)

    @T.prim_func
    def main(
        A: T.Tensor((N), dtype),  # Input tensor A
        B: T.Tensor((N), dtype),  # Input tensor B
        C: T.Tensor((N), dtype),  # Output tensor C = A + B
        shape: T.int32,           # Actual size (used for handling tail cases if N is not divisible by block_N)
    ):
        # Launch kernel with `n_num` parallel threads on the NPU
        with T.Kernel(n_num, is_npu=True) as (cid, _):
            # Allocate on-chip Unified Buffer (UB) for local computation
            A_VEC = T.alloc_ub((block_N), dtype)
            B_VEC = T.alloc_ub((block_N), dtype)
            C_VEC = T.alloc_ub((block_N), dtype)

            # Calculate the starting index for this thread
            start_idx = cid * block_N
            # Compute remaining elements from this start index to the end of the tensor
            remaining = shape - start_idx
            # Determine how many elements this thread should actually process (handles tail)
            tail_size = T.min(block_N, remaining)

            # Copy data from global memory (A, B) into on-chip buffers (A_VEC, B_VEC)
            T.copy(A[start_idx], A_VEC, [tail_size])
            T.copy(B[start_idx], B_VEC, [tail_size])

            # Perform vector addition on the NPU using low-level NPU IR instruction
            T.npuir_add(A_VEC, B_VEC, C_VEC)

            # Write the result back from on-chip buffer (C_VEC) to global memory (C)
            T.copy(C_VEC, C[start_idx], [tail_size])

    return main

def test_vec_add():
    """
    Test function to validate the vector addition kernel.
    Compares the result of the custom TileLang kernel against PyTorch's native addition.
    """
    # Set the target NPU device (device ID 6 in this case)
    torch.npu.set_device(6)

    # Instantiate the vector addition kernel for the full sequence length (single block)
    func = vec_add(seq_len, seq_len)

    # Compile the TileLang function to NPU IR for execution on the NPU
    compiled_kernel = tilelang.compile(func, target="npuir")

    # Create random input tensors on the NPU
    v1 = torch.randn(size=[seq_len], dtype=eval("torch." + dtype)).npu()
    v2 = torch.randn(size=[seq_len], dtype=eval("torch." + dtype)).npu()
    v3 = torch.zeros(size=[seq_len], dtype=eval("torch." + dtype)).npu()  # Output buffer

    # Compute reference result using PyTorch's native addition (on NPU)
    y_ref = v1 + v2

    # Launch the compiled TileLang kernel
    compiled_kernel(v1, v2, v3, seq_len)

    # Print both results for visual comparison (should be nearly identical)
    print("Reference result (PyTorch):")
    print(y_ref)
    print("TileLang kernel result:")
    print(v3)

if __name__ == "__main__":
    test_vec_add()
  ```

<!-- ### Dive Deep into TileLang Beyond GEMM

In addition to GEMM, we provide a variety of examples to showcase the versatility and power of TileLang, including:

- [Dequantize GEMM](./examples/dequantize_gemm/): Achieve high-performance dequantization by **fine-grained control over per-thread operations**, with many features now adopted as default behaviors in [BitBLAS](https://github.com/microsoft/BitBLAS), which utilizing magic layout transformation and intrins to accelerate dequantize gemm.
- [FlashAttention](./examples/flash_attention/): Enable cross-operator fusion with simple and intuitive syntax, and we also provide an example of auto tuning.
- [LinearAttention](./examples/linear_attention/): Examples include RetNet and Mamba implementations.
- [Convolution](./examples/convolution/): Implementations of Convolution with IM2Col. -->

## Roadmap
<!-- <div align="center"> -->
<img src="./images/roadmap.png" alt="æ’å›¾3" />

<!-- 
Check our [tilelang v0.2.0 release plan](https://github.com/tile-ai/tilelang/issues/79) for upcoming features.

---

TileLang has now been used in project [BitBLAS](https://github.com/microsoft/BitBLAS) and [AttentionEngine](https://github.com/microsoft/AttentionEngine). -->

## Join the Discussion

Welcome to join our Discord community for discussions, support, and collaboration!

[![Join our Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?logo=discord&style=for-the-badge)](https://discord.gg/TUrHyJnKPG)

## Acknowledgements

Peking University Kunpeng & Ascend Center for Excellence in Science, Education, Innovation

